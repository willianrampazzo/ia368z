{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 - exercicio de classificar 3 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer classificação de 3 classes usando rede neural convolucional.\n",
    "Não utilizar o pacote sklearn. Apenas o Keras e o NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não utilizar a função de acompanhamento de gráficos durante o treinamento.\n",
    "\n",
    "Gerar uma figura mosaic que contenha as 5 imagens de classificação correta de menor probabilidade de predição.\n",
    "\n",
    "Gerar esta figura com o nome: cifar_fig.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.regularizers as reg\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "K.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('/etc/jupyterhub/ia368z_2s2017/datasets/cifar10-redux.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data['X_train']\n",
    "y = data['y_train']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 3, 32, 32), (2000,), (500, 3, 32, 32), (500,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), dtype('uint8'), dtype('uint8'), dtype('int64'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype, y.dtype, X_test.dtype, y_test.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255.\n",
    "\n",
    "# transforma labels em 0, 1 e 2, ao invés de 3, 4 e 5\n",
    "y = y - 3\n",
    "y_test = y_test - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do iterator para a geração de imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definindo iterator do treino\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    #featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    #samplewise_center=False,  # set each sample mean to 0\n",
    "    #featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    #samplewise_std_normalization=False,  # divide each input by its std\n",
    "    #zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    #vertical_flip=False,  # randomly flip images\n",
    "    shear_range=0.2,        # sorteio entre 0 e 0.2 distribuição uniforme\n",
    "    zoom_range=0.2,         # sorteio entre 0 e 0.2\n",
    "    data_format='channels_first')\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "#train_datagen.fit(X)\n",
    "\n",
    "# definindo iterator do teste\n",
    "validate_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrando alguns exemplos de imagens geradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADECAYAAACGNXroAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa5b35c8dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Criação do generator\n",
    "generator = train_datagen.flow(X, y, batch_size=8)\n",
    "\n",
    "X_img, y_img = generator.next()\n",
    "\n",
    "fig = plt.figure(figsize=(6,3))\n",
    "for i, img in enumerate(X_img):\n",
    "    fig.add_subplot(2, 4, i+1)\n",
    "    plt.imshow(img.reshape(3,32,32).transpose((1,2,0)))\n",
    "\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação dos dados de treinamento em treinamento e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1605, 3, 32, 32)\n",
      "(395, 3, 32, 32)\n",
      "(1605,)\n",
      "(395,)\n"
     ]
    }
   ],
   "source": [
    "# numero de amostras\n",
    "nb_data = X.shape[0]\n",
    "\n",
    "# semente fixa para dar reproducibilidade\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "\n",
    "msk = np.random.rand(nb_data) < 0.80\n",
    "X_train = X[msk]\n",
    "X_validate = X[~msk]\n",
    "y_train = y[msk]\n",
    "y_validate = y[~msk]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_validate.shape)\n",
    "print(y_train.shape)\n",
    "print(y_validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforma o vetor de labels para o formato de one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classes = 3\n",
    "\n",
    "y_oh = np_utils.to_categorical(y, nb_classes)\n",
    "y_train_oh = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_validate_oh = np_utils.to_categorical(y_validate, nb_classes)\n",
    "y_test_oh = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construíndo a CNN com o Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 30, 30)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 30, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 28, 28)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 28, 28)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 12, 12)        27696     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 48, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 48, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 48, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1728)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               221312    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 268,787\n",
      "Trainable params: 268,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 32, 32 # Dimensões das imagens\n",
    "input_shape = (3, img_rows, img_cols)\n",
    "\n",
    "# Definindo a rede\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(48,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Aqui os features deixam de ser imagens\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Dense(nb_classes, kernel_regularizer=reg.l2(0.025)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilando a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = 'categorical_crossentropy'\n",
    "#opt = RMSprop()\n",
    "opt = Adam()\n",
    "\n",
    "model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='my_cifar_dataplus.hdf5', monitor='val_acc', verbose=1, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "lrreduction = ReduceLROnPlateau(monitor='val_acc', patience=7, verbose=1, factor=0.8, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 1.1715 - acc: 0.3444Epoch 00000: val_acc improved from -inf to 0.36398, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 4s - loss: 1.1710 - acc: 0.3450 - val_loss: 1.1225 - val_acc: 0.3640\n",
      "Epoch 2/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 1.1139 - acc: 0.3637Epoch 00001: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 1.1137 - acc: 0.3620 - val_loss: 1.1023 - val_acc: 0.3457\n",
      "Epoch 3/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 1.1023 - acc: 0.3444Epoch 00002: val_acc improved from 0.36398 to 0.36587, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 1.1021 - acc: 0.3470 - val_loss: 1.0991 - val_acc: 0.3659\n",
      "Epoch 4/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 1.0858 - acc: 0.4036Epoch 00003: val_acc improved from 0.36587 to 0.51105, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 1.0861 - acc: 0.4045 - val_loss: 1.0745 - val_acc: 0.5110\n",
      "Epoch 5/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 1.0556 - acc: 0.4359Epoch 00004: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 1.0553 - acc: 0.4360 - val_loss: 1.0514 - val_acc: 0.3996\n",
      "Epoch 6/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 1.0141 - acc: 0.4774Epoch 00005: val_acc improved from 0.51105 to 0.54928, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 1.0137 - acc: 0.4765 - val_loss: 0.9465 - val_acc: 0.5493\n",
      "Epoch 7/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.9853 - acc: 0.5112Epoch 00006: val_acc improved from 0.54928 to 0.56455, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.9881 - acc: 0.5105 - val_loss: 0.9126 - val_acc: 0.5645\n",
      "Epoch 8/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.9529 - acc: 0.5255Epoch 00007: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.9514 - acc: 0.5265 - val_loss: 0.9510 - val_acc: 0.5285\n",
      "Epoch 9/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.9446 - acc: 0.5321Epoch 00008: val_acc improved from 0.56455 to 0.57604, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.9426 - acc: 0.5325 - val_loss: 0.8627 - val_acc: 0.5760\n",
      "Epoch 10/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.9417 - acc: 0.5365Epoch 00009: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.9409 - acc: 0.5365 - val_loss: 0.8933 - val_acc: 0.5715\n",
      "Epoch 11/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.9266 - acc: 0.5394Epoch 00010: val_acc improved from 0.57604 to 0.57903, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.9245 - acc: 0.5405 - val_loss: 0.8880 - val_acc: 0.5790\n",
      "Epoch 12/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.9129 - acc: 0.5572Epoch 00011: val_acc improved from 0.57903 to 0.60985, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.9122 - acc: 0.5605 - val_loss: 0.8578 - val_acc: 0.6098\n",
      "Epoch 13/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.9112 - acc: 0.5531Epoch 00012: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.9093 - acc: 0.5570 - val_loss: 0.8389 - val_acc: 0.5971\n",
      "Epoch 14/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.9130 - acc: 0.5711Epoch 00013: val_acc improved from 0.60985 to 0.62579, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.9145 - acc: 0.5710 - val_loss: 0.8267 - val_acc: 0.6258\n",
      "Epoch 15/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.8888 - acc: 0.5797Epoch 00014: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.8879 - acc: 0.5815 - val_loss: 0.8382 - val_acc: 0.6031\n",
      "Epoch 16/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.8877 - acc: 0.5770Epoch 00015: val_acc improved from 0.62579 to 0.62831, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.8846 - acc: 0.5810 - val_loss: 0.8158 - val_acc: 0.6283\n",
      "Epoch 17/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.8899 - acc: 0.5742Epoch 00016: val_acc improved from 0.62831 to 0.63571, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.8857 - acc: 0.5765 - val_loss: 0.8089 - val_acc: 0.6357\n",
      "Epoch 18/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.8905 - acc: 0.5682Epoch 00017: val_acc improved from 0.63571 to 0.64767, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.8896 - acc: 0.5695 - val_loss: 0.7992 - val_acc: 0.6477\n",
      "Epoch 19/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.8761 - acc: 0.5843Epoch 00018: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.8775 - acc: 0.5845 - val_loss: 0.8020 - val_acc: 0.6447\n",
      "Epoch 20/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.8671 - acc: 0.6010Epoch 00019: val_acc improved from 0.64767 to 0.66004, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.8681 - acc: 0.6000 - val_loss: 0.7926 - val_acc: 0.6600\n",
      "Epoch 21/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.8465 - acc: 0.6070Epoch 00020: val_acc improved from 0.66004 to 0.66719, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.8464 - acc: 0.6070 - val_loss: 0.7572 - val_acc: 0.6672\n",
      "Epoch 22/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.8693 - acc: 0.5954Epoch 00021: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.8722 - acc: 0.5925 - val_loss: 0.7660 - val_acc: 0.6524\n",
      "Epoch 23/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.8691 - acc: 0.6000Epoch 00022: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.8684 - acc: 0.6010 - val_loss: 0.7961 - val_acc: 0.6607\n",
      "Epoch 24/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.8288 - acc: 0.6091Epoch 00023: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.8343 - acc: 0.6065 - val_loss: 0.7684 - val_acc: 0.6659\n",
      "Epoch 25/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.8377 - acc: 0.6151Epoch 00024: val_acc improved from 0.66719 to 0.67884, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.8367 - acc: 0.6160 - val_loss: 0.7423 - val_acc: 0.6788\n",
      "Epoch 26/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.8358 - acc: 0.5944Epoch 00025: val_acc improved from 0.67884 to 0.69553, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.8358 - acc: 0.5955 - val_loss: 0.7196 - val_acc: 0.6955\n",
      "Epoch 27/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.8145 - acc: 0.6332Epoch 00026: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.8239 - acc: 0.6270 - val_loss: 0.7077 - val_acc: 0.6944\n",
      "Epoch 28/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.8273 - acc: 0.6313Epoch 00027: val_acc improved from 0.69553 to 0.70734, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.8243 - acc: 0.6325 - val_loss: 0.6998 - val_acc: 0.7073\n",
      "Epoch 29/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.8290 - acc: 0.6242Epoch 00028: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.8302 - acc: 0.6235 - val_loss: 0.7296 - val_acc: 0.6984\n",
      "Epoch 30/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.8020 - acc: 0.6369Epoch 00029: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7987 - acc: 0.6390 - val_loss: 0.7156 - val_acc: 0.7042\n",
      "Epoch 31/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.8136 - acc: 0.6380Epoch 00030: val_acc improved from 0.70734 to 0.71054, saving model to cifar10.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 2s - loss: 0.8161 - acc: 0.6380 - val_loss: 0.6853 - val_acc: 0.7105\n",
      "Epoch 32/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.7931 - acc: 0.6561Epoch 00031: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7926 - acc: 0.6570 - val_loss: 0.6843 - val_acc: 0.7084\n",
      "Epoch 33/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.8068 - acc: 0.6338Epoch 00032: val_acc improved from 0.71054 to 0.72639, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.8078 - acc: 0.6325 - val_loss: 0.6781 - val_acc: 0.7264\n",
      "Epoch 34/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.7954 - acc: 0.6378Epoch 00033: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7914 - acc: 0.6410 - val_loss: 0.6819 - val_acc: 0.7165\n",
      "Epoch 35/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.7806 - acc: 0.6574Epoch 00034: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7818 - acc: 0.6580 - val_loss: 0.6937 - val_acc: 0.6902\n",
      "Epoch 36/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.7675 - acc: 0.6589Epoch 00035: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7684 - acc: 0.6580 - val_loss: 0.6535 - val_acc: 0.7226\n",
      "Epoch 37/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.8032 - acc: 0.6289Epoch 00036: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.8023 - acc: 0.6275 - val_loss: 0.6621 - val_acc: 0.7264\n",
      "Epoch 38/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.7618 - acc: 0.6646Epoch 00037: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7642 - acc: 0.6645 - val_loss: 0.6650 - val_acc: 0.7223\n",
      "Epoch 39/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.7828 - acc: 0.6538Epoch 00038: val_acc improved from 0.72639 to 0.73864, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7832 - acc: 0.6530 - val_loss: 0.6343 - val_acc: 0.7386\n",
      "Epoch 40/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.7659 - acc: 0.6544Epoch 00039: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7650 - acc: 0.6540 - val_loss: 0.6342 - val_acc: 0.7369\n",
      "Epoch 41/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.7822 - acc: 0.6520Epoch 00040: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7833 - acc: 0.6520 - val_loss: 0.6921 - val_acc: 0.7177\n",
      "Epoch 42/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.7765 - acc: 0.6547Epoch 00041: val_acc improved from 0.73864 to 0.74606, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7730 - acc: 0.6570 - val_loss: 0.6480 - val_acc: 0.7461\n",
      "Epoch 43/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.7544 - acc: 0.6764Epoch 00042: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7572 - acc: 0.6765 - val_loss: 0.6318 - val_acc: 0.7353\n",
      "Epoch 44/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.7668 - acc: 0.6645Epoch 00043: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7667 - acc: 0.6645 - val_loss: 0.6311 - val_acc: 0.7399\n",
      "Epoch 45/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.7573 - acc: 0.6693Epoch 00044: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7651 - acc: 0.6660 - val_loss: 0.6352 - val_acc: 0.7335\n",
      "Epoch 46/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.7721 - acc: 0.6603Epoch 00045: val_acc improved from 0.74606 to 0.74811, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7723 - acc: 0.6600 - val_loss: 0.6174 - val_acc: 0.7481\n",
      "Epoch 47/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.7665 - acc: 0.6682Epoch 00046: val_acc improved from 0.74811 to 0.74905, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7684 - acc: 0.6665 - val_loss: 0.6321 - val_acc: 0.7491\n",
      "Epoch 48/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.7678 - acc: 0.6629Epoch 00047: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7696 - acc: 0.6620 - val_loss: 0.6477 - val_acc: 0.7344\n",
      "Epoch 49/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.7363 - acc: 0.6833Epoch 00048: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7403 - acc: 0.6810 - val_loss: 0.6606 - val_acc: 0.7350\n",
      "Epoch 50/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.7655 - acc: 0.6768Epoch 00049: val_acc improved from 0.74905 to 0.75252, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7671 - acc: 0.6755 - val_loss: 0.6050 - val_acc: 0.7525\n",
      "Epoch 51/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.7503 - acc: 0.6741Epoch 00050: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7493 - acc: 0.6755 - val_loss: 0.6069 - val_acc: 0.7456\n",
      "Epoch 52/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.7179 - acc: 0.6788Epoch 00051: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7146 - acc: 0.6820 - val_loss: 0.6093 - val_acc: 0.7379\n",
      "Epoch 53/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.7256 - acc: 0.6779Epoch 00052: val_acc improved from 0.75252 to 0.76448, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7234 - acc: 0.6790 - val_loss: 0.5618 - val_acc: 0.7645\n",
      "Epoch 54/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.7526 - acc: 0.6927Epoch 00053: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7570 - acc: 0.6880 - val_loss: 0.6092 - val_acc: 0.7628\n",
      "Epoch 55/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.7272 - acc: 0.6746Epoch 00054: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7268 - acc: 0.6745 - val_loss: 0.5976 - val_acc: 0.7593\n",
      "Epoch 56/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.7450 - acc: 0.6714Epoch 00055: val_acc improved from 0.76448 to 0.77251, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7437 - acc: 0.6715 - val_loss: 0.5847 - val_acc: 0.7725\n",
      "Epoch 57/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.7136 - acc: 0.6812Epoch 00056: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7139 - acc: 0.6820 - val_loss: 0.5855 - val_acc: 0.7713\n",
      "Epoch 58/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.6726Epoch 00057: val_acc improved from 0.77251 to 0.77944, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7335 - acc: 0.6740 - val_loss: 0.5571 - val_acc: 0.7794\n",
      "Epoch 59/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.7228 - acc: 0.7066Epoch 00058: val_acc improved from 0.77944 to 0.78038, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7248 - acc: 0.7055 - val_loss: 0.5640 - val_acc: 0.7804\n",
      "Epoch 60/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.7319 - acc: 0.6888Epoch 00059: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7318 - acc: 0.6895 - val_loss: 0.6110 - val_acc: 0.7733\n",
      "Epoch 61/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.7281 - acc: 0.6785Epoch 00060: val_acc improved from 0.78038 to 0.78220, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7277 - acc: 0.6785 - val_loss: 0.5622 - val_acc: 0.7822\n",
      "Epoch 62/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.7143 - acc: 0.6943Epoch 00061: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7138 - acc: 0.6935 - val_loss: 0.5639 - val_acc: 0.7763\n",
      "Epoch 63/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.6886 - acc: 0.6948Epoch 00062: val_acc improved from 0.78220 to 0.79534, saving model to cifar10.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 2s - loss: 0.6863 - acc: 0.6975 - val_loss: 0.5355 - val_acc: 0.7953\n",
      "Epoch 64/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.7228 - acc: 0.6797Epoch 00063: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7169 - acc: 0.6825 - val_loss: 0.5273 - val_acc: 0.7928\n",
      "Epoch 65/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.7113 - acc: 0.6918Epoch 00064: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7099 - acc: 0.6930 - val_loss: 0.5537 - val_acc: 0.7767\n",
      "Epoch 66/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.7137 - acc: 0.6934Epoch 00065: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7171 - acc: 0.6915 - val_loss: 0.5433 - val_acc: 0.7933\n",
      "Epoch 67/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.7077 - acc: 0.6935Epoch 00066: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7064 - acc: 0.6940 - val_loss: 0.5571 - val_acc: 0.7827\n",
      "Epoch 68/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.7061 - acc: 0.6932Epoch 00067: val_acc improved from 0.79534 to 0.79676, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7027 - acc: 0.6955 - val_loss: 0.5636 - val_acc: 0.7968\n",
      "Epoch 69/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.7159 - acc: 0.6955Epoch 00068: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.7156 - acc: 0.6955 - val_loss: 0.5584 - val_acc: 0.7831\n",
      "Epoch 70/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.6919 - acc: 0.7051Epoch 00069: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6915 - acc: 0.7050 - val_loss: 0.5526 - val_acc: 0.7780\n",
      "Epoch 71/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.6924 - acc: 0.6948Epoch 00070: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6921 - acc: 0.6955 - val_loss: 0.5660 - val_acc: 0.7875\n",
      "Epoch 72/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.7047 - acc: 0.6893Epoch 00071: val_acc improved from 0.79676 to 0.80290, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7036 - acc: 0.6915 - val_loss: 0.5172 - val_acc: 0.8029\n",
      "Epoch 73/500\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 0.6943 - acc: 0.7120Epoch 00072: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6927 - acc: 0.7140 - val_loss: 0.5443 - val_acc: 0.7909\n",
      "Epoch 74/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.6969 - acc: 0.7026Epoch 00073: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6950 - acc: 0.7020 - val_loss: 0.5354 - val_acc: 0.8007\n",
      "Epoch 75/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.6779 - acc: 0.6995Epoch 00074: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6862 - acc: 0.6940 - val_loss: 0.5685 - val_acc: 0.7883\n",
      "Epoch 76/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.6738 - acc: 0.7092Epoch 00075: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6744 - acc: 0.7090 - val_loss: 0.5399 - val_acc: 0.8007\n",
      "Epoch 77/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.7062 - acc: 0.6881Epoch 00076: val_acc improved from 0.80290 to 0.80856, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.7059 - acc: 0.6900 - val_loss: 0.5084 - val_acc: 0.8086\n",
      "Epoch 78/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.7046Epoch 00077: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6704 - acc: 0.7060 - val_loss: 0.4986 - val_acc: 0.8021\n",
      "Epoch 79/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.6938 - acc: 0.6893Epoch 00078: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6939 - acc: 0.6895 - val_loss: 0.5427 - val_acc: 0.7709\n",
      "Epoch 80/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.7008 - acc: 0.7005Epoch 00079: val_acc improved from 0.80856 to 0.81990, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.6997 - acc: 0.7015 - val_loss: 0.5359 - val_acc: 0.8199\n",
      "Epoch 81/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.6890 - acc: 0.7040Epoch 00080: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6865 - acc: 0.7055 - val_loss: 0.5178 - val_acc: 0.8106\n",
      "Epoch 82/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.6739 - acc: 0.7083Epoch 00081: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6727 - acc: 0.7085 - val_loss: 0.4920 - val_acc: 0.8108\n",
      "Epoch 83/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.6564 - acc: 0.7197Epoch 00082: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6565 - acc: 0.7200 - val_loss: 0.4971 - val_acc: 0.8130\n",
      "Epoch 84/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.6608 - acc: 0.7148Epoch 00083: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6621 - acc: 0.7120 - val_loss: 0.5262 - val_acc: 0.8089\n",
      "Epoch 85/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.6532 - acc: 0.7180Epoch 00084: val_acc improved from 0.81990 to 0.82336, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.6552 - acc: 0.7180 - val_loss: 0.4744 - val_acc: 0.8234\n",
      "Epoch 86/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.6364 - acc: 0.7260Epoch 00085: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6430 - acc: 0.7250 - val_loss: 0.4916 - val_acc: 0.8141\n",
      "Epoch 87/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.7119Epoch 00086: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6758 - acc: 0.7105 - val_loss: 0.5455 - val_acc: 0.7890\n",
      "Epoch 88/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.6612 - acc: 0.7217Epoch 00087: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6613 - acc: 0.7220 - val_loss: 0.5162 - val_acc: 0.7982\n",
      "Epoch 89/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.6528 - acc: 0.7119Epoch 00088: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6536 - acc: 0.7115 - val_loss: 0.4626 - val_acc: 0.8196\n",
      "Epoch 90/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.6662 - acc: 0.7128Epoch 00089: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6647 - acc: 0.7130 - val_loss: 0.4671 - val_acc: 0.8213\n",
      "Epoch 91/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.6675 - acc: 0.7157Epoch 00090: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6668 - acc: 0.7155 - val_loss: 0.5044 - val_acc: 0.8045\n",
      "Epoch 92/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.6562 - acc: 0.7253Epoch 00091: val_acc improved from 0.82336 to 0.83076, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.6532 - acc: 0.7265 - val_loss: 0.4570 - val_acc: 0.8308\n",
      "Epoch 93/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.7186Epoch 00092: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6659 - acc: 0.7180 - val_loss: 0.4880 - val_acc: 0.8176\n",
      "Epoch 94/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.6361 - acc: 0.7337Epoch 00093: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6377 - acc: 0.7320 - val_loss: 0.4745 - val_acc: 0.8282\n",
      "Epoch 95/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6719 - acc: 0.7111Epoch 00094: val_acc improved from 0.83076 to 0.83548, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.6708 - acc: 0.7115 - val_loss: 0.4440 - val_acc: 0.8355\n",
      "Epoch 96/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.6473 - acc: 0.7231Epoch 00095: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6414 - acc: 0.7275 - val_loss: 0.4664 - val_acc: 0.8330\n",
      "Epoch 97/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.6514 - acc: 0.7173Epoch 00096: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6515 - acc: 0.7170 - val_loss: 0.5005 - val_acc: 0.8243\n",
      "Epoch 98/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.6637 - acc: 0.7203Epoch 00097: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6631 - acc: 0.7195 - val_loss: 0.4840 - val_acc: 0.8103\n",
      "Epoch 99/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.6246 - acc: 0.7264Epoch 00098: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6229 - acc: 0.7270 - val_loss: 0.4600 - val_acc: 0.8284\n",
      "Epoch 100/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.7356Epoch 00099: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6467 - acc: 0.7325 - val_loss: 0.4504 - val_acc: 0.8341\n",
      "Epoch 101/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.6183 - acc: 0.7409Epoch 00100: val_acc improved from 0.83548 to 0.84044, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.6195 - acc: 0.7395 - val_loss: 0.4496 - val_acc: 0.8404\n",
      "Epoch 102/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.6438 - acc: 0.7302Epoch 00101: val_acc improved from 0.84044 to 0.85469, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.6439 - acc: 0.7305 - val_loss: 0.4190 - val_acc: 0.8547\n",
      "Epoch 103/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.6386 - acc: 0.7294Epoch 00102: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6387 - acc: 0.7280 - val_loss: 0.4349 - val_acc: 0.8456\n",
      "Epoch 104/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.6275 - acc: 0.7323Epoch 00103: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6260 - acc: 0.7325 - val_loss: 0.4490 - val_acc: 0.8373\n",
      "Epoch 105/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.6571 - acc: 0.7303Epoch 00104: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6577 - acc: 0.7305 - val_loss: 0.4449 - val_acc: 0.8465\n",
      "Epoch 106/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.6436 - acc: 0.7354Epoch 00105: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6425 - acc: 0.7360 - val_loss: 0.4801 - val_acc: 0.8298\n",
      "Epoch 107/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.6288 - acc: 0.7302Epoch 00106: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6288 - acc: 0.7320 - val_loss: 0.4530 - val_acc: 0.8246\n",
      "Epoch 108/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.6215 - acc: 0.7318Epoch 00107: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6215 - acc: 0.7310 - val_loss: 0.4349 - val_acc: 0.8355\n",
      "Epoch 109/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.6319 - acc: 0.7323Epoch 00108: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6291 - acc: 0.7325 - val_loss: 0.4351 - val_acc: 0.8377\n",
      "Epoch 110/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.6272 - acc: 0.7379Epoch 00109: val_acc did not improve\n",
      "\n",
      "Epoch 00109: reducing learning rate to 0.000800000037997961.\n",
      "200/200 [==============================] - 2s - loss: 0.6263 - acc: 0.7385 - val_loss: 0.4114 - val_acc: 0.8435\n",
      "Epoch 111/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6289 - acc: 0.7307Epoch 00110: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6298 - acc: 0.7300 - val_loss: 0.4072 - val_acc: 0.8527\n",
      "Epoch 112/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6261 - acc: 0.7402Epoch 00111: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6243 - acc: 0.7415 - val_loss: 0.4091 - val_acc: 0.8489\n",
      "Epoch 113/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.6055 - acc: 0.7342Epoch 00112: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6102 - acc: 0.7335 - val_loss: 0.4283 - val_acc: 0.8544\n",
      "Epoch 114/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.7438Epoch 00113: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6124 - acc: 0.7440 - val_loss: 0.4450 - val_acc: 0.8459\n",
      "Epoch 115/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.6033 - acc: 0.7438Epoch 00114: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6065 - acc: 0.7440 - val_loss: 0.4754 - val_acc: 0.8112\n",
      "Epoch 116/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5983 - acc: 0.7553Epoch 00115: val_acc improved from 0.85469 to 0.86146, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5970 - acc: 0.7560 - val_loss: 0.3931 - val_acc: 0.8615\n",
      "Epoch 117/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.5885 - acc: 0.7552Epoch 00116: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5934 - acc: 0.7535 - val_loss: 0.4372 - val_acc: 0.8478\n",
      "Epoch 118/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.6146 - acc: 0.7510Epoch 00117: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6100 - acc: 0.7535 - val_loss: 0.3998 - val_acc: 0.8508\n",
      "Epoch 119/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.6057 - acc: 0.7433Epoch 00118: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6082 - acc: 0.7410 - val_loss: 0.4315 - val_acc: 0.8491\n",
      "Epoch 120/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5866 - acc: 0.7588Epoch 00119: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5867 - acc: 0.7590 - val_loss: 0.3960 - val_acc: 0.8607\n",
      "Epoch 121/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5873 - acc: 0.7545Epoch 00120: val_acc improved from 0.86146 to 0.86398, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5856 - acc: 0.7555 - val_loss: 0.3972 - val_acc: 0.8640\n",
      "Epoch 122/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5930 - acc: 0.7477Epoch 00121: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5925 - acc: 0.7470 - val_loss: 0.4175 - val_acc: 0.8593\n",
      "Epoch 123/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.5921 - acc: 0.7653Epoch 00122: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5939 - acc: 0.7645 - val_loss: 0.4137 - val_acc: 0.8387\n",
      "Epoch 124/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.6037 - acc: 0.7462Epoch 00123: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.6021 - acc: 0.7465 - val_loss: 0.4028 - val_acc: 0.8615\n",
      "Epoch 125/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.7641Epoch 00124: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5745 - acc: 0.7645 - val_loss: 0.3990 - val_acc: 0.8588\n",
      "Epoch 126/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.5949 - acc: 0.7399Epoch 00125: val_acc improved from 0.86398 to 0.86697, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5986 - acc: 0.7385 - val_loss: 0.3864 - val_acc: 0.8670\n",
      "Epoch 127/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.6025 - acc: 0.7508Epoch 00126: val_acc improved from 0.86697 to 0.86774, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5981 - acc: 0.7530 - val_loss: 0.3939 - val_acc: 0.8677\n",
      "Epoch 128/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5931 - acc: 0.7442Epoch 00127: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 2s - loss: 0.5960 - acc: 0.7435 - val_loss: 0.3922 - val_acc: 0.8641\n",
      "Epoch 129/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.5699 - acc: 0.7601Epoch 00128: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5709 - acc: 0.7585 - val_loss: 0.3906 - val_acc: 0.8566\n",
      "Epoch 130/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5679 - acc: 0.7631Epoch 00129: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5658 - acc: 0.7645 - val_loss: 0.3819 - val_acc: 0.8637\n",
      "Epoch 131/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5784 - acc: 0.7665Epoch 00130: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5840 - acc: 0.7665 - val_loss: 0.4034 - val_acc: 0.8640\n",
      "Epoch 132/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.5798 - acc: 0.7637Epoch 00131: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5824 - acc: 0.7615 - val_loss: 0.3897 - val_acc: 0.8629\n",
      "Epoch 133/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.5663 - acc: 0.7617Epoch 00132: val_acc improved from 0.86774 to 0.87437, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5725 - acc: 0.7595 - val_loss: 0.3669 - val_acc: 0.8744\n",
      "Epoch 134/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5704 - acc: 0.7533Epoch 00133: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5679 - acc: 0.7550 - val_loss: 0.3758 - val_acc: 0.8684\n",
      "Epoch 135/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.7723Epoch 00134: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5698 - acc: 0.7725 - val_loss: 0.3710 - val_acc: 0.8725\n",
      "Epoch 136/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.7626Epoch 00135: val_acc improved from 0.87437 to 0.87957, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5582 - acc: 0.7635 - val_loss: 0.3497 - val_acc: 0.8796\n",
      "Epoch 137/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5990 - acc: 0.7503Epoch 00136: val_acc improved from 0.87957 to 0.88098, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.6006 - acc: 0.7495 - val_loss: 0.3856 - val_acc: 0.8810\n",
      "Epoch 138/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.5710 - acc: 0.7684Epoch 00137: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5771 - acc: 0.7655 - val_loss: 0.3662 - val_acc: 0.8677\n",
      "Epoch 139/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5721 - acc: 0.7616Epoch 00138: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5697 - acc: 0.7630 - val_loss: 0.3646 - val_acc: 0.8700\n",
      "Epoch 140/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.7628Epoch 00139: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5475 - acc: 0.7630 - val_loss: 0.3801 - val_acc: 0.8536\n",
      "Epoch 141/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5885 - acc: 0.7503Epoch 00140: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5901 - acc: 0.7500 - val_loss: 0.3894 - val_acc: 0.8706\n",
      "Epoch 142/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.5590 - acc: 0.7622Epoch 00141: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5564 - acc: 0.7640 - val_loss: 0.3694 - val_acc: 0.8739\n",
      "Epoch 143/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5597 - acc: 0.7734Epoch 00142: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5590 - acc: 0.7740 - val_loss: 0.3684 - val_acc: 0.8621\n",
      "Epoch 144/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.7698Epoch 00143: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5545 - acc: 0.7690 - val_loss: 0.3568 - val_acc: 0.8744\n",
      "Epoch 145/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.5649 - acc: 0.7569Epoch 00144: val_acc did not improve\n",
      "\n",
      "Epoch 00144: reducing learning rate to 0.0006400000303983689.\n",
      "200/200 [==============================] - 2s - loss: 0.5636 - acc: 0.7580 - val_loss: 0.3798 - val_acc: 0.8676\n",
      "Epoch 146/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5648 - acc: 0.7711Epoch 00145: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5644 - acc: 0.7710 - val_loss: 0.3920 - val_acc: 0.8725\n",
      "Epoch 147/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.7833Epoch 00146: val_acc improved from 0.88098 to 0.88224, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5450 - acc: 0.7820 - val_loss: 0.3585 - val_acc: 0.8822\n",
      "Epoch 148/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5640 - acc: 0.7613Epoch 00147: val_acc improved from 0.88224 to 0.88696, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5625 - acc: 0.7620 - val_loss: 0.3543 - val_acc: 0.8870\n",
      "Epoch 149/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5580 - acc: 0.7613Epoch 00148: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5594 - acc: 0.7600 - val_loss: 0.3334 - val_acc: 0.8796\n",
      "Epoch 150/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.7761Epoch 00149: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5571 - acc: 0.7775 - val_loss: 0.3594 - val_acc: 0.8725\n",
      "Epoch 151/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7874Epoch 00150: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5484 - acc: 0.7885 - val_loss: 0.3748 - val_acc: 0.8745\n",
      "Epoch 152/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.7778Epoch 00151: val_acc improved from 0.88696 to 0.89200, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5313 - acc: 0.7790 - val_loss: 0.3178 - val_acc: 0.8920\n",
      "Epoch 153/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5723 - acc: 0.7528Epoch 00152: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5709 - acc: 0.7535 - val_loss: 0.3685 - val_acc: 0.8801\n",
      "Epoch 154/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.7773Epoch 00153: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5348 - acc: 0.7780 - val_loss: 0.3180 - val_acc: 0.8833\n",
      "Epoch 155/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5266 - acc: 0.7879Epoch 00154: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5271 - acc: 0.7870 - val_loss: 0.3066 - val_acc: 0.8789\n",
      "Epoch 156/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.7862Epoch 00155: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5381 - acc: 0.7855 - val_loss: 0.3372 - val_acc: 0.8903\n",
      "Epoch 157/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.5512 - acc: 0.7630Epoch 00156: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5466 - acc: 0.7660 - val_loss: 0.3740 - val_acc: 0.8864\n",
      "Epoch 158/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.7804Epoch 00157: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5393 - acc: 0.7815 - val_loss: 0.3360 - val_acc: 0.8906\n",
      "Epoch 159/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7843Epoch 00158: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5206 - acc: 0.7845 - val_loss: 0.3293 - val_acc: 0.8800\n",
      "Epoch 160/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.7774Epoch 00159: val_acc improved from 0.89200 to 0.89279, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5616 - acc: 0.7755 - val_loss: 0.3234 - val_acc: 0.8928\n",
      "Epoch 161/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.7816Epoch 00160: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5379 - acc: 0.7815 - val_loss: 0.3451 - val_acc: 0.8892\n",
      "Epoch 162/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.7919Epoch 00161: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5042 - acc: 0.7900 - val_loss: 0.3437 - val_acc: 0.8764\n",
      "Epoch 163/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.7789Epoch 00162: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5555 - acc: 0.7780 - val_loss: 0.3284 - val_acc: 0.8827\n",
      "Epoch 164/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.7837Epoch 00163: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5414 - acc: 0.7830 - val_loss: 0.3909 - val_acc: 0.8520\n",
      "Epoch 165/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.7787Epoch 00164: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5381 - acc: 0.7810 - val_loss: 0.3523 - val_acc: 0.8846\n",
      "Epoch 166/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.7754Epoch 00165: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5446 - acc: 0.7750 - val_loss: 0.3259 - val_acc: 0.8903\n",
      "Epoch 167/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5397 - acc: 0.7833Epoch 00166: val_acc improved from 0.89279 to 0.89688, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5426 - acc: 0.7820 - val_loss: 0.3206 - val_acc: 0.8969\n",
      "Epoch 168/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.5622 - acc: 0.7753Epoch 00167: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5623 - acc: 0.7760 - val_loss: 0.3463 - val_acc: 0.8913\n",
      "Epoch 169/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7872Epoch 00168: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5219 - acc: 0.7860 - val_loss: 0.3239 - val_acc: 0.8967\n",
      "Epoch 170/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.7889Epoch 00169: val_acc improved from 0.89688 to 0.90035, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5320 - acc: 0.7885 - val_loss: 0.3394 - val_acc: 0.9003\n",
      "Epoch 171/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5050 - acc: 0.7884Epoch 00170: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5059 - acc: 0.7880 - val_loss: 0.3122 - val_acc: 0.8981\n",
      "Epoch 172/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.5032 - acc: 0.7906Epoch 00171: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5079 - acc: 0.7910 - val_loss: 0.3023 - val_acc: 0.8955\n",
      "Epoch 173/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.5164 - acc: 0.7906Epoch 00172: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5182 - acc: 0.7895 - val_loss: 0.3321 - val_acc: 0.8870\n",
      "Epoch 174/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.5271 - acc: 0.7876Epoch 00173: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5327 - acc: 0.7835 - val_loss: 0.3020 - val_acc: 0.8985\n",
      "Epoch 175/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7838Epoch 00174: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5330 - acc: 0.7825 - val_loss: 0.3241 - val_acc: 0.8944\n",
      "Epoch 176/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.5265 - acc: 0.7867Epoch 00175: val_acc improved from 0.90035 to 0.90215, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5264 - acc: 0.7860 - val_loss: 0.2991 - val_acc: 0.9021\n",
      "Epoch 177/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5034 - acc: 0.7909Epoch 00176: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5058 - acc: 0.7905 - val_loss: 0.3219 - val_acc: 0.8833\n",
      "Epoch 178/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5312 - acc: 0.7717Epoch 00177: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5314 - acc: 0.7720 - val_loss: 0.3217 - val_acc: 0.8909\n",
      "Epoch 179/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.7844Epoch 00178: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5024 - acc: 0.7845 - val_loss: 0.3175 - val_acc: 0.8878\n",
      "Epoch 180/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.5155 - acc: 0.7885Epoch 00179: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5144 - acc: 0.7910 - val_loss: 0.3226 - val_acc: 0.8997\n",
      "Epoch 181/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.7868Epoch 00180: val_acc improved from 0.90215 to 0.90885, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5181 - acc: 0.7860 - val_loss: 0.2955 - val_acc: 0.9088\n",
      "Epoch 182/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.5231 - acc: 0.7851Epoch 00181: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5275 - acc: 0.7825 - val_loss: 0.3620 - val_acc: 0.8855\n",
      "Epoch 183/500\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 0.5179 - acc: 0.7848Epoch 00182: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5204 - acc: 0.7835 - val_loss: 0.3056 - val_acc: 0.9014\n",
      "Epoch 184/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.7814Epoch 00183: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5236 - acc: 0.7820 - val_loss: 0.3187 - val_acc: 0.9016\n",
      "Epoch 185/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.7849Epoch 00184: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5011 - acc: 0.7855 - val_loss: 0.3153 - val_acc: 0.9014\n",
      "Epoch 186/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.5083 - acc: 0.7917Epoch 00185: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5054 - acc: 0.7945 - val_loss: 0.2879 - val_acc: 0.9041\n",
      "Epoch 187/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.5331 - acc: 0.7745Epoch 00186: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5352 - acc: 0.7735 - val_loss: 0.3175 - val_acc: 0.9068\n",
      "Epoch 188/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.7848Epoch 00187: val_acc improved from 0.90885 to 0.91193, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5198 - acc: 0.7860 - val_loss: 0.3032 - val_acc: 0.9119\n",
      "Epoch 189/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.7808Epoch 00188: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5197 - acc: 0.7815 - val_loss: 0.3574 - val_acc: 0.8755\n",
      "Epoch 190/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.7836Epoch 00189: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5325 - acc: 0.7830 - val_loss: 0.3122 - val_acc: 0.8989\n",
      "Epoch 191/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.5114 - acc: 0.7881Epoch 00190: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5106 - acc: 0.7910 - val_loss: 0.3065 - val_acc: 0.9073\n",
      "Epoch 192/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5215 - acc: 0.7799Epoch 00191: val_acc improved from 0.91193 to 0.91751, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5212 - acc: 0.7795 - val_loss: 0.2831 - val_acc: 0.9175\n",
      "Epoch 193/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/200 [============================>.] - ETA: 0s - loss: 0.4932 - acc: 0.7985Epoch 00192: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4991 - acc: 0.7950 - val_loss: 0.3037 - val_acc: 0.8915\n",
      "Epoch 194/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.5266 - acc: 0.7898Epoch 00193: val_acc improved from 0.91751 to 0.91861, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.5256 - acc: 0.7900 - val_loss: 0.2895 - val_acc: 0.9186\n",
      "Epoch 195/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8146Epoch 00194: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4776 - acc: 0.8150 - val_loss: 0.2859 - val_acc: 0.9127\n",
      "Epoch 196/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4833 - acc: 0.8057Epoch 00195: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4874 - acc: 0.8050 - val_loss: 0.3120 - val_acc: 0.8977\n",
      "Epoch 197/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.5259 - acc: 0.7870Epoch 00196: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5240 - acc: 0.7865 - val_loss: 0.2879 - val_acc: 0.9076\n",
      "Epoch 198/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.7889Epoch 00197: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5183 - acc: 0.7890 - val_loss: 0.2871 - val_acc: 0.9148\n",
      "Epoch 199/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.7930Epoch 00198: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5029 - acc: 0.7920 - val_loss: 0.3001 - val_acc: 0.9113\n",
      "Epoch 200/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4918 - acc: 0.7934Epoch 00199: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4945 - acc: 0.7920 - val_loss: 0.2902 - val_acc: 0.9046\n",
      "Epoch 201/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.7933Epoch 00200: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5074 - acc: 0.7905 - val_loss: 0.2988 - val_acc: 0.9111\n",
      "Epoch 202/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.5072 - acc: 0.7990Epoch 00201: val_acc did not improve\n",
      "\n",
      "Epoch 00201: reducing learning rate to 0.0005120000336319208.\n",
      "200/200 [==============================] - 2s - loss: 0.5061 - acc: 0.7985 - val_loss: 0.2955 - val_acc: 0.9044\n",
      "Epoch 203/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4819 - acc: 0.8108Epoch 00202: val_acc improved from 0.91861 to 0.91872, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4823 - acc: 0.8110 - val_loss: 0.2724 - val_acc: 0.9187\n",
      "Epoch 204/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.5178 - acc: 0.7958Epoch 00203: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5188 - acc: 0.7950 - val_loss: 0.2915 - val_acc: 0.9159\n",
      "Epoch 205/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.7863Epoch 00204: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4913 - acc: 0.7875 - val_loss: 0.3022 - val_acc: 0.9092\n",
      "Epoch 206/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4830 - acc: 0.8015Epoch 00205: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4812 - acc: 0.8030 - val_loss: 0.2736 - val_acc: 0.9155\n",
      "Epoch 207/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4836 - acc: 0.7913Epoch 00206: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4844 - acc: 0.7915 - val_loss: 0.2794 - val_acc: 0.9096\n",
      "Epoch 208/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.4671 - acc: 0.8062Epoch 00207: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4658 - acc: 0.8060 - val_loss: 0.2715 - val_acc: 0.9159\n",
      "Epoch 209/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.8077Epoch 00208: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4848 - acc: 0.8050 - val_loss: 0.2828 - val_acc: 0.9131\n",
      "Epoch 210/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8030Epoch 00209: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.5017 - acc: 0.8030 - val_loss: 0.2605 - val_acc: 0.9077\n",
      "Epoch 211/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4852 - acc: 0.8061Epoch 00210: val_acc did not improve\n",
      "\n",
      "Epoch 00210: reducing learning rate to 0.00040960004553198815.\n",
      "200/200 [==============================] - 2s - loss: 0.4875 - acc: 0.8045 - val_loss: 0.2756 - val_acc: 0.9173\n",
      "Epoch 212/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4515 - acc: 0.8161Epoch 00211: val_acc improved from 0.91872 to 0.92065, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4567 - acc: 0.8135 - val_loss: 0.2603 - val_acc: 0.9207\n",
      "Epoch 213/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4657 - acc: 0.8147Epoch 00212: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4689 - acc: 0.8135 - val_loss: 0.2717 - val_acc: 0.9158\n",
      "Epoch 214/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8026Epoch 00213: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4783 - acc: 0.8030 - val_loss: 0.2781 - val_acc: 0.9151\n",
      "Epoch 215/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4647 - acc: 0.8223Epoch 00214: val_acc improved from 0.92065 to 0.92254, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4673 - acc: 0.8190 - val_loss: 0.2648 - val_acc: 0.9225\n",
      "Epoch 216/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.8045Epoch 00215: val_acc improved from 0.92254 to 0.92774, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4871 - acc: 0.8040 - val_loss: 0.2719 - val_acc: 0.9277\n",
      "Epoch 217/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4734 - acc: 0.8071Epoch 00216: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4711 - acc: 0.8070 - val_loss: 0.2780 - val_acc: 0.9194\n",
      "Epoch 218/500\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 0.4693 - acc: 0.7990Epoch 00217: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4740 - acc: 0.7985 - val_loss: 0.2570 - val_acc: 0.9222\n",
      "Epoch 219/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4735 - acc: 0.8156Epoch 00218: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4740 - acc: 0.8150 - val_loss: 0.2621 - val_acc: 0.9191\n",
      "Epoch 220/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4486 - acc: 0.8156Epoch 00219: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4478 - acc: 0.8165 - val_loss: 0.2519 - val_acc: 0.9224\n",
      "Epoch 221/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4690 - acc: 0.8036Epoch 00220: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4690 - acc: 0.8030 - val_loss: 0.2521 - val_acc: 0.9169\n",
      "Epoch 222/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.8250Epoch 00221: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4443 - acc: 0.8255 - val_loss: 0.2502 - val_acc: 0.9224\n",
      "Epoch 223/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4896 - acc: 0.7929Epoch 00222: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4882 - acc: 0.7940 - val_loss: 0.2637 - val_acc: 0.9222\n",
      "Epoch 224/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4748 - acc: 0.8107Epoch 00223: val_acc did not improve\n",
      "\n",
      "Epoch 00223: reducing learning rate to 0.00032768002711236477.\n",
      "200/200 [==============================] - 2s - loss: 0.4736 - acc: 0.8110 - val_loss: 0.2601 - val_acc: 0.9203\n",
      "Epoch 225/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4539 - acc: 0.8206Epoch 00224: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4517 - acc: 0.8215 - val_loss: 0.2544 - val_acc: 0.9246\n",
      "Epoch 226/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4514 - acc: 0.8281Epoch 00225: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4522 - acc: 0.8280 - val_loss: 0.2582 - val_acc: 0.9194\n",
      "Epoch 227/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4633 - acc: 0.8106Epoch 00226: val_acc improved from 0.92774 to 0.93042, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4632 - acc: 0.8115 - val_loss: 0.2483 - val_acc: 0.9304\n",
      "Epoch 228/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4601 - acc: 0.8166Epoch 00227: val_acc improved from 0.93042 to 0.93388, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4564 - acc: 0.8190 - val_loss: 0.2480 - val_acc: 0.9339\n",
      "Epoch 229/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.8231Epoch 00228: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4493 - acc: 0.8240 - val_loss: 0.2518 - val_acc: 0.9235\n",
      "Epoch 230/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8139Epoch 00229: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4505 - acc: 0.8140 - val_loss: 0.2533 - val_acc: 0.9229\n",
      "Epoch 231/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4504 - acc: 0.8167Epoch 00230: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4514 - acc: 0.8170 - val_loss: 0.2388 - val_acc: 0.9277\n",
      "Epoch 232/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.8388Epoch 00231: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4175 - acc: 0.8385 - val_loss: 0.2473 - val_acc: 0.9246\n",
      "Epoch 233/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.8208Epoch 00232: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4415 - acc: 0.8225 - val_loss: 0.2394 - val_acc: 0.9277\n",
      "Epoch 234/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4717 - acc: 0.8136Epoch 00233: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4708 - acc: 0.8135 - val_loss: 0.2560 - val_acc: 0.9266\n",
      "Epoch 235/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4527 - acc: 0.8307Epoch 00234: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4522 - acc: 0.8305 - val_loss: 0.2470 - val_acc: 0.9301\n",
      "Epoch 236/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4359 - acc: 0.8245Epoch 00235: val_acc did not improve\n",
      "\n",
      "Epoch 00235: reducing learning rate to 0.0002621440216898918.\n",
      "200/200 [==============================] - 2s - loss: 0.4355 - acc: 0.8240 - val_loss: 0.2362 - val_acc: 0.9285\n",
      "Epoch 237/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.8175Epoch 00236: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4730 - acc: 0.8155 - val_loss: 0.2411 - val_acc: 0.9332\n",
      "Epoch 238/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4575 - acc: 0.8197Epoch 00237: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4563 - acc: 0.8200 - val_loss: 0.2394 - val_acc: 0.9315\n",
      "Epoch 239/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4472 - acc: 0.8276Epoch 00238: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4450 - acc: 0.8280 - val_loss: 0.2512 - val_acc: 0.9293\n",
      "Epoch 240/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4515 - acc: 0.8191Epoch 00239: val_acc improved from 0.93388 to 0.93829, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4520 - acc: 0.8185 - val_loss: 0.2351 - val_acc: 0.9383\n",
      "Epoch 241/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4517 - acc: 0.8237Epoch 00240: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4500 - acc: 0.8235 - val_loss: 0.2539 - val_acc: 0.9230\n",
      "Epoch 242/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.8172Epoch 00241: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4533 - acc: 0.8175 - val_loss: 0.2496 - val_acc: 0.9241\n",
      "Epoch 243/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8179Epoch 00242: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4387 - acc: 0.8185 - val_loss: 0.2312 - val_acc: 0.9366\n",
      "Epoch 244/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4420 - acc: 0.8328Epoch 00243: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4389 - acc: 0.8335 - val_loss: 0.2233 - val_acc: 0.9328\n",
      "Epoch 245/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4325 - acc: 0.8244Epoch 00244: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4314 - acc: 0.8255 - val_loss: 0.2198 - val_acc: 0.9334\n",
      "Epoch 246/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4427 - acc: 0.8202Epoch 00245: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4472 - acc: 0.8190 - val_loss: 0.2358 - val_acc: 0.9277\n",
      "Epoch 247/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4362 - acc: 0.8297Epoch 00246: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4353 - acc: 0.8305 - val_loss: 0.2269 - val_acc: 0.9271\n",
      "Epoch 248/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4574 - acc: 0.8132Epoch 00247: val_acc did not improve\n",
      "\n",
      "Epoch 00247: reducing learning rate to 0.00020971521735191345.\n",
      "200/200 [==============================] - 2s - loss: 0.4541 - acc: 0.8160 - val_loss: 0.2399 - val_acc: 0.9310\n",
      "Epoch 249/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8236Epoch 00248: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4209 - acc: 0.8235 - val_loss: 0.2353 - val_acc: 0.9304\n",
      "Epoch 250/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8386Epoch 00249: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4172 - acc: 0.8380 - val_loss: 0.2220 - val_acc: 0.9329\n",
      "Epoch 251/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.8241Epoch 00250: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4368 - acc: 0.8240 - val_loss: 0.2209 - val_acc: 0.9320\n",
      "Epoch 252/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4238 - acc: 0.8372Epoch 00251: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4284 - acc: 0.8355 - val_loss: 0.2211 - val_acc: 0.9378\n",
      "Epoch 253/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4382 - acc: 0.8340Epoch 00252: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4360 - acc: 0.8350 - val_loss: 0.2250 - val_acc: 0.9375\n",
      "Epoch 254/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4314 - acc: 0.8292Epoch 00253: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4284 - acc: 0.8310 - val_loss: 0.2227 - val_acc: 0.9347\n",
      "Epoch 255/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8371Epoch 00254: val_acc did not improve\n",
      "\n",
      "Epoch 00254: reducing learning rate to 0.00016777217388153076.\n",
      "200/200 [==============================] - 2s - loss: 0.4197 - acc: 0.8365 - val_loss: 0.2263 - val_acc: 0.9318\n",
      "Epoch 256/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4361 - acc: 0.8230Epoch 00255: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4362 - acc: 0.8225 - val_loss: 0.2326 - val_acc: 0.9348\n",
      "Epoch 257/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4421 - acc: 0.8264Epoch 00256: val_acc improved from 0.93829 to 0.94049, saving model to cifar10.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 2s - loss: 0.4398 - acc: 0.8285 - val_loss: 0.2188 - val_acc: 0.9405\n",
      "Epoch 258/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4107 - acc: 0.8266Epoch 00257: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4091 - acc: 0.8285 - val_loss: 0.2220 - val_acc: 0.9339\n",
      "Epoch 259/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8308Epoch 00258: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4267 - acc: 0.8295 - val_loss: 0.2161 - val_acc: 0.9378\n",
      "Epoch 260/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4383 - acc: 0.8292Epoch 00259: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4403 - acc: 0.8280 - val_loss: 0.2298 - val_acc: 0.9358\n",
      "Epoch 261/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4280 - acc: 0.8313Epoch 00260: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4275 - acc: 0.8305 - val_loss: 0.2282 - val_acc: 0.9312\n",
      "Epoch 262/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4232 - acc: 0.8328Epoch 00261: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4249 - acc: 0.8340 - val_loss: 0.2185 - val_acc: 0.9369\n",
      "Epoch 263/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8258Epoch 00262: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4349 - acc: 0.8260 - val_loss: 0.2224 - val_acc: 0.9400\n",
      "Epoch 264/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8371Epoch 00263: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4098 - acc: 0.8370 - val_loss: 0.2177 - val_acc: 0.9381\n",
      "Epoch 265/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4191 - acc: 0.8330Epoch 00264: val_acc did not improve\n",
      "\n",
      "Epoch 00264: reducing learning rate to 0.00013421773910522462.\n",
      "200/200 [==============================] - 2s - loss: 0.4226 - acc: 0.8330 - val_loss: 0.2165 - val_acc: 0.9381\n",
      "Epoch 266/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4207 - acc: 0.8303Epoch 00265: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4198 - acc: 0.8310 - val_loss: 0.2213 - val_acc: 0.9377\n",
      "Epoch 267/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4257 - acc: 0.8365Epoch 00266: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4236 - acc: 0.8380 - val_loss: 0.2190 - val_acc: 0.9358\n",
      "Epoch 268/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8439Epoch 00267: val_acc improved from 0.94049 to 0.94065, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.3915 - acc: 0.8440 - val_loss: 0.2072 - val_acc: 0.9406\n",
      "Epoch 269/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4261 - acc: 0.8171Epoch 00268: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4257 - acc: 0.8195 - val_loss: 0.2117 - val_acc: 0.9364\n",
      "Epoch 270/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.4194 - acc: 0.8323Epoch 00269: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4192 - acc: 0.8325 - val_loss: 0.2103 - val_acc: 0.9386\n",
      "Epoch 271/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8362Epoch 00270: val_acc improved from 0.94065 to 0.94254, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4127 - acc: 0.8365 - val_loss: 0.2061 - val_acc: 0.9425\n",
      "Epoch 272/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8318Epoch 00271: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4151 - acc: 0.8315 - val_loss: 0.2130 - val_acc: 0.9383\n",
      "Epoch 273/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.8242Epoch 00272: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4402 - acc: 0.8260 - val_loss: 0.2181 - val_acc: 0.9366\n",
      "Epoch 274/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4199 - acc: 0.8231Epoch 00273: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4204 - acc: 0.8225 - val_loss: 0.2174 - val_acc: 0.9395\n",
      "Epoch 275/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.8318Epoch 00274: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4177 - acc: 0.8325 - val_loss: 0.2211 - val_acc: 0.9348\n",
      "Epoch 276/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4034 - acc: 0.8376Epoch 00275: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4048 - acc: 0.8355 - val_loss: 0.2117 - val_acc: 0.9370\n",
      "Epoch 277/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4221 - acc: 0.8388Epoch 00276: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4219 - acc: 0.8375 - val_loss: 0.2147 - val_acc: 0.9373\n",
      "Epoch 278/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8437Epoch 00277: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4154 - acc: 0.8445 - val_loss: 0.2085 - val_acc: 0.9366\n",
      "Epoch 279/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4059 - acc: 0.8482Epoch 00278: val_acc did not improve\n",
      "\n",
      "Epoch 00278: reducing learning rate to 0.00010737419361248613.\n",
      "200/200 [==============================] - 2s - loss: 0.4040 - acc: 0.8500 - val_loss: 0.2210 - val_acc: 0.9345\n",
      "Epoch 280/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.3985 - acc: 0.8404Epoch 00279: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3992 - acc: 0.8405 - val_loss: 0.2119 - val_acc: 0.9402\n",
      "Epoch 281/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8307Epoch 00280: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4217 - acc: 0.8300 - val_loss: 0.2164 - val_acc: 0.9405\n",
      "Epoch 282/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8325Epoch 00281: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4065 - acc: 0.8330 - val_loss: 0.2055 - val_acc: 0.9406\n",
      "Epoch 283/500\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 0.4134 - acc: 0.8361Epoch 00282: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4072 - acc: 0.8390 - val_loss: 0.2113 - val_acc: 0.9355\n",
      "Epoch 284/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3996 - acc: 0.8465Epoch 00283: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4029 - acc: 0.8445 - val_loss: 0.2054 - val_acc: 0.9375\n",
      "Epoch 285/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4105 - acc: 0.8375Epoch 00284: val_acc improved from 0.94254 to 0.94285, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4112 - acc: 0.8370 - val_loss: 0.2097 - val_acc: 0.9429\n",
      "Epoch 286/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4000 - acc: 0.8396Epoch 00285: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4007 - acc: 0.8415 - val_loss: 0.2060 - val_acc: 0.9422\n",
      "Epoch 287/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4154 - acc: 0.8326Epoch 00286: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4199 - acc: 0.8325 - val_loss: 0.2086 - val_acc: 0.9425\n",
      "Epoch 288/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8327Epoch 00287: val_acc improved from 0.94285 to 0.94413, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4072 - acc: 0.8320 - val_loss: 0.2031 - val_acc: 0.9441\n",
      "Epoch 289/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8399Epoch 00288: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4136 - acc: 0.8400 - val_loss: 0.2068 - val_acc: 0.9408\n",
      "Epoch 290/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8332Epoch 00289: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4222 - acc: 0.8335 - val_loss: 0.2095 - val_acc: 0.9435\n",
      "Epoch 291/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4126 - acc: 0.8385Epoch 00290: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4203 - acc: 0.8350 - val_loss: 0.2056 - val_acc: 0.9410\n",
      "Epoch 292/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4081 - acc: 0.8394Epoch 00291: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4073 - acc: 0.8400 - val_loss: 0.2109 - val_acc: 0.9406\n",
      "Epoch 293/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8397Epoch 00292: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4044 - acc: 0.8405 - val_loss: 0.2083 - val_acc: 0.9399\n",
      "Epoch 294/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4047 - acc: 0.8396Epoch 00293: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4050 - acc: 0.8385 - val_loss: 0.2065 - val_acc: 0.9406\n",
      "Epoch 295/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8332Epoch 00294: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4035 - acc: 0.8330 - val_loss: 0.2075 - val_acc: 0.9441\n",
      "Epoch 296/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4199 - acc: 0.8443Epoch 00295: val_acc did not improve\n",
      "\n",
      "Epoch 00295: reducing learning rate to 8.589935605414213e-05.\n",
      "200/200 [==============================] - 2s - loss: 0.4197 - acc: 0.8450 - val_loss: 0.2059 - val_acc: 0.9397\n",
      "Epoch 297/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.3951 - acc: 0.8378Epoch 00296: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3931 - acc: 0.8385 - val_loss: 0.2017 - val_acc: 0.9438\n",
      "Epoch 298/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4183 - acc: 0.8443Epoch 00297: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4232 - acc: 0.8410 - val_loss: 0.2053 - val_acc: 0.9405\n",
      "Epoch 299/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4031 - acc: 0.8378Epoch 00298: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4035 - acc: 0.8380 - val_loss: 0.2096 - val_acc: 0.9422\n",
      "Epoch 300/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8434Epoch 00299: val_acc improved from 0.94413 to 0.94429, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.3849 - acc: 0.8435 - val_loss: 0.2036 - val_acc: 0.9443\n",
      "Epoch 301/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8427Epoch 00300: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3860 - acc: 0.8425 - val_loss: 0.2014 - val_acc: 0.9422\n",
      "Epoch 302/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4229 - acc: 0.8325Epoch 00301: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4212 - acc: 0.8330 - val_loss: 0.2070 - val_acc: 0.9416\n",
      "Epoch 303/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8387Epoch 00302: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3799 - acc: 0.8385 - val_loss: 0.2017 - val_acc: 0.9425\n",
      "Epoch 304/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.8490Epoch 00303: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3837 - acc: 0.8500 - val_loss: 0.1987 - val_acc: 0.9435\n",
      "Epoch 305/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8360Epoch 00304: val_acc improved from 0.94429 to 0.94474, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.3859 - acc: 0.8375 - val_loss: 0.1987 - val_acc: 0.9447\n",
      "Epoch 306/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.8429Epoch 00305: val_acc improved from 0.94474 to 0.94553, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4239 - acc: 0.8435 - val_loss: 0.2009 - val_acc: 0.9455\n",
      "Epoch 307/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3991 - acc: 0.8414Epoch 00306: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4009 - acc: 0.8400 - val_loss: 0.1986 - val_acc: 0.9432\n",
      "Epoch 308/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.8320Epoch 00307: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4288 - acc: 0.8325 - val_loss: 0.2029 - val_acc: 0.9436\n",
      "Epoch 309/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.3857 - acc: 0.8474Epoch 00308: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3912 - acc: 0.8460 - val_loss: 0.2050 - val_acc: 0.9444\n",
      "Epoch 310/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.4237 - acc: 0.8344Epoch 00309: val_acc improved from 0.94553 to 0.94695, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.4199 - acc: 0.8360 - val_loss: 0.2003 - val_acc: 0.9469\n",
      "Epoch 311/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8369Epoch 00310: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3939 - acc: 0.8365 - val_loss: 0.2035 - val_acc: 0.9437\n",
      "Epoch 312/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8313Epoch 00311: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4066 - acc: 0.8315 - val_loss: 0.2044 - val_acc: 0.9438\n",
      "Epoch 313/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.4137 - acc: 0.8313Epoch 00312: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4112 - acc: 0.8320 - val_loss: 0.2045 - val_acc: 0.9452\n",
      "Epoch 314/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.3999 - acc: 0.8430Epoch 00313: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4003 - acc: 0.8425 - val_loss: 0.2016 - val_acc: 0.9438\n",
      "Epoch 315/500\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 0.3998 - acc: 0.8393Epoch 00314: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4046 - acc: 0.8375 - val_loss: 0.2003 - val_acc: 0.9405\n",
      "Epoch 316/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8543Epoch 00315: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3905 - acc: 0.8550 - val_loss: 0.2004 - val_acc: 0.9454\n",
      "Epoch 317/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8314Epoch 00316: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4129 - acc: 0.8315 - val_loss: 0.2012 - val_acc: 0.9418\n",
      "Epoch 318/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8379Epoch 00317: val_acc did not improve\n",
      "\n",
      "Epoch 00317: reducing learning rate to 6.871948717162013e-05.\n",
      "200/200 [==============================] - 2s - loss: 0.4030 - acc: 0.8385 - val_loss: 0.2021 - val_acc: 0.9457\n",
      "Epoch 319/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8352Epoch 00318: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4159 - acc: 0.8340 - val_loss: 0.2069 - val_acc: 0.9429\n",
      "Epoch 320/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4104 - acc: 0.8470Epoch 00319: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4108 - acc: 0.8465 - val_loss: 0.2030 - val_acc: 0.9432\n",
      "Epoch 321/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8459Epoch 00320: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3984 - acc: 0.8455 - val_loss: 0.2049 - val_acc: 0.9413\n",
      "Epoch 322/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8451Epoch 00321: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 2s - loss: 0.3899 - acc: 0.8435 - val_loss: 0.2002 - val_acc: 0.9414\n",
      "Epoch 323/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4138 - acc: 0.8404Epoch 00322: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4115 - acc: 0.8415 - val_loss: 0.1970 - val_acc: 0.9437\n",
      "Epoch 324/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8414Epoch 00323: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3998 - acc: 0.8415 - val_loss: 0.1989 - val_acc: 0.9451\n",
      "Epoch 325/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3991 - acc: 0.8288Epoch 00324: val_acc improved from 0.94695 to 0.94742, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.3971 - acc: 0.8300 - val_loss: 0.1975 - val_acc: 0.9474\n",
      "Epoch 326/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.3948 - acc: 0.8432Epoch 00325: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4026 - acc: 0.8405 - val_loss: 0.1970 - val_acc: 0.9425\n",
      "Epoch 327/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8444Epoch 00326: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3900 - acc: 0.8450 - val_loss: 0.1944 - val_acc: 0.9467\n",
      "Epoch 328/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8357Epoch 00327: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4082 - acc: 0.8360 - val_loss: 0.1938 - val_acc: 0.9457\n",
      "Epoch 329/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8372Epoch 00328: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3890 - acc: 0.8370 - val_loss: 0.1998 - val_acc: 0.9419\n",
      "Epoch 330/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8416Epoch 00329: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3928 - acc: 0.8420 - val_loss: 0.1932 - val_acc: 0.9466\n",
      "Epoch 331/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8470Epoch 00330: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3927 - acc: 0.8480 - val_loss: 0.1929 - val_acc: 0.9440\n",
      "Epoch 332/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8366Epoch 00331: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4118 - acc: 0.8385 - val_loss: 0.1923 - val_acc: 0.9444\n",
      "Epoch 333/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.4182 - acc: 0.8415Epoch 00332: val_acc did not improve\n",
      "\n",
      "Epoch 00332: reducing learning rate to 5.497558740898967e-05.\n",
      "200/200 [==============================] - 2s - loss: 0.4149 - acc: 0.8430 - val_loss: 0.1935 - val_acc: 0.9463\n",
      "Epoch 334/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8448Epoch 00333: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3996 - acc: 0.8455 - val_loss: 0.1959 - val_acc: 0.9451\n",
      "Epoch 335/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8474Epoch 00334: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3808 - acc: 0.8470 - val_loss: 0.1946 - val_acc: 0.9466\n",
      "Epoch 336/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.3885 - acc: 0.8401Epoch 00335: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3888 - acc: 0.8405 - val_loss: 0.1919 - val_acc: 0.9446\n",
      "Epoch 337/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8487Epoch 00336: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3905 - acc: 0.8495 - val_loss: 0.1904 - val_acc: 0.9438\n",
      "Epoch 338/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8479Epoch 00337: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4078 - acc: 0.8500 - val_loss: 0.1936 - val_acc: 0.9469\n",
      "Epoch 339/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.4016 - acc: 0.8426Epoch 00338: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3971 - acc: 0.8440 - val_loss: 0.1943 - val_acc: 0.9462\n",
      "Epoch 340/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8465Epoch 00339: val_acc improved from 0.94742 to 0.94915, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.3899 - acc: 0.8455 - val_loss: 0.1910 - val_acc: 0.9491\n",
      "Epoch 341/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8503Epoch 00340: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3964 - acc: 0.8495 - val_loss: 0.1931 - val_acc: 0.9465\n",
      "Epoch 342/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8465Epoch 00341: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3870 - acc: 0.8455 - val_loss: 0.1921 - val_acc: 0.9480\n",
      "Epoch 343/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.3885 - acc: 0.8523Epoch 00342: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3870 - acc: 0.8515 - val_loss: 0.1959 - val_acc: 0.9425\n",
      "Epoch 344/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8495Epoch 00343: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3975 - acc: 0.8490 - val_loss: 0.1935 - val_acc: 0.9471\n",
      "Epoch 345/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4038 - acc: 0.8397Epoch 00344: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4030 - acc: 0.8400 - val_loss: 0.1957 - val_acc: 0.9454\n",
      "Epoch 346/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4150 - acc: 0.8318Epoch 00345: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4152 - acc: 0.8325 - val_loss: 0.1972 - val_acc: 0.9466\n",
      "Epoch 347/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.3755 - acc: 0.8562Epoch 00346: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3763 - acc: 0.8550 - val_loss: 0.1926 - val_acc: 0.9485\n",
      "Epoch 348/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3957 - acc: 0.8492Epoch 00347: val_acc did not improve\n",
      "\n",
      "Epoch 00347: reducing learning rate to 4.398046876303852e-05.\n",
      "200/200 [==============================] - 2s - loss: 0.3941 - acc: 0.8500 - val_loss: 0.1920 - val_acc: 0.9476\n",
      "Epoch 349/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8592Epoch 00348: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3776 - acc: 0.8595 - val_loss: 0.1923 - val_acc: 0.9457\n",
      "Epoch 350/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.3926 - acc: 0.8440Epoch 00349: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3886 - acc: 0.8450 - val_loss: 0.1916 - val_acc: 0.9452\n",
      "Epoch 351/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8359Epoch 00350: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4062 - acc: 0.8370 - val_loss: 0.1911 - val_acc: 0.9478\n",
      "Epoch 352/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8340Epoch 00351: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4022 - acc: 0.8360 - val_loss: 0.1941 - val_acc: 0.9482\n",
      "Epoch 353/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.3834 - acc: 0.8420Epoch 00352: val_acc improved from 0.94915 to 0.94946, saving model to cifar10.hdf5\n",
      "200/200 [==============================] - 2s - loss: 0.3822 - acc: 0.8425 - val_loss: 0.1924 - val_acc: 0.9495\n",
      "Epoch 354/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8518Epoch 00353: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3873 - acc: 0.8520 - val_loss: 0.1935 - val_acc: 0.9471\n",
      "Epoch 355/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.3697 - acc: 0.8575Epoch 00354: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3700 - acc: 0.8580 - val_loss: 0.1918 - val_acc: 0.9471\n",
      "Epoch 356/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8413Epoch 00355: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4049 - acc: 0.8405 - val_loss: 0.1957 - val_acc: 0.9441\n",
      "Epoch 357/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.3864 - acc: 0.8461Epoch 00356: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3853 - acc: 0.8470 - val_loss: 0.1906 - val_acc: 0.9485\n",
      "Epoch 358/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.3735 - acc: 0.8518Epoch 00357: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3782 - acc: 0.8505 - val_loss: 0.1899 - val_acc: 0.9474\n",
      "Epoch 359/500\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.3705 - acc: 0.8492Epoch 00358: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3729 - acc: 0.8485 - val_loss: 0.1898 - val_acc: 0.9473\n",
      "Epoch 360/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8480Epoch 00359: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3877 - acc: 0.8475 - val_loss: 0.1925 - val_acc: 0.9447\n",
      "Epoch 361/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.8402Epoch 00360: val_acc did not improve\n",
      "\n",
      "Epoch 00360: reducing learning rate to 3.518437442835421e-05.\n",
      "200/200 [==============================] - 2s - loss: 0.3804 - acc: 0.8405 - val_loss: 0.1930 - val_acc: 0.9457\n",
      "Epoch 362/500\n",
      "195/200 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8487Epoch 00361: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3796 - acc: 0.8490 - val_loss: 0.1913 - val_acc: 0.9478\n",
      "Epoch 363/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8444Epoch 00362: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3932 - acc: 0.8440 - val_loss: 0.1922 - val_acc: 0.9487\n",
      "Epoch 364/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8443Epoch 00363: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3981 - acc: 0.8450 - val_loss: 0.1897 - val_acc: 0.9487\n",
      "Epoch 365/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3734 - acc: 0.8533Epoch 00364: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3741 - acc: 0.8530 - val_loss: 0.1937 - val_acc: 0.9466\n",
      "Epoch 366/500\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8444Epoch 00365: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3993 - acc: 0.8435 - val_loss: 0.1877 - val_acc: 0.9460\n",
      "Epoch 367/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8384Epoch 00366: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.4025 - acc: 0.8390 - val_loss: 0.1917 - val_acc: 0.9477\n",
      "Epoch 368/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.3958 - acc: 0.8432Epoch 00367: val_acc did not improve\n",
      "\n",
      "Epoch 00367: reducing learning rate to 2.8147498960606756e-05.\n",
      "200/200 [==============================] - 2s - loss: 0.3950 - acc: 0.8450 - val_loss: 0.1937 - val_acc: 0.9427\n",
      "Epoch 369/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8482Epoch 00368: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3812 - acc: 0.8490 - val_loss: 0.1892 - val_acc: 0.9462\n",
      "Epoch 370/500\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8472Epoch 00369: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3905 - acc: 0.8455 - val_loss: 0.1884 - val_acc: 0.9451\n",
      "Epoch 371/500\n",
      "194/200 [============================>.] - ETA: 0s - loss: 0.3597 - acc: 0.8593Epoch 00370: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3593 - acc: 0.8590 - val_loss: 0.1867 - val_acc: 0.9457\n",
      "Epoch 372/500\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3941 - acc: 0.8439Epoch 00371: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3942 - acc: 0.8440 - val_loss: 0.1908 - val_acc: 0.9455\n",
      "Epoch 373/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.3833 - acc: 0.8536Epoch 00372: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3843 - acc: 0.8535 - val_loss: 0.1894 - val_acc: 0.9462\n",
      "Epoch 374/500\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.3716 - acc: 0.8594Epoch 00373: val_acc did not improve\n",
      "200/200 [==============================] - 2s - loss: 0.3674 - acc: 0.8615 - val_loss: 0.1859 - val_acc: 0.9482\n",
      "Epoch 00373: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5a7fd8da0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "epochs = 500\n",
    "\n",
    "model.fit_generator(train_datagen.flow(X, y_oh,\n",
    "                                       batch_size=batch_size),\n",
    "                    steps_per_epoch=X.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(validate_datagen.flow(X, y_oh)),\n",
    "                    validation_steps=X.shape[0] // batch_size,\n",
    "                    callbacks=[checkpointer, earlystopper, lrreduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apaga modelo atual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carrega modelo salvo em disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('my_cifar_dataplus.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do treinamento no conjunto de testes com o melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/500 [========================>.....] - ETA: 0s\n",
      "[INFO] accuracy on the test data set: 75.40% [0.68947]\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_oh, batch_size=10)\n",
    "print(\"\\n[INFO] accuracy on the test data set: {:.2f}% [{:.5f}]\".format(accuracy * 100, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
